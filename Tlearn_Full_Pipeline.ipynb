{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tlearn008/-tlearn-dashboard/blob/main/Tlearn_Full_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Install requirements\n",
        "!pip install openai pytrends gspread oauth2client replicate soundfile \\\n",
        "  git+https://github.com/suno-ai/bark.git \\\n",
        "  python-telegram-bot --quiet\n",
        "!sudo apt-get install -y ffmpeg\n",
        "\n",
        "# üóÉÔ∏è Create Google Sheets + YouTube credential files\n",
        "service_account = '''{\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"tlearn-automation\",\n",
        "  \"private_key_id\": \"d69ba204f9ffb36f8a664855d56f6f2d8ee912b9\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDHYg6odafJtUqb\\\\njG6zxvxsev4bF60mb4jXTXGIu0jKLwTDlpF2n6s7qP5cor0pKkmDj3wWSiBrnsKv\\\\naL0gXA/O+RZdc29yv5CHGruxlhY9o38Mputhlzc8N+Ofnm2JhJTLejwePO4CES1M\\\\nfjVoS7bF1ekcehJ45TDb2CFc6gnTYXkhLWMp+wZWSZvucZviNRs2lxLiB5BGsff9\\\\n4w+d0DyyBWke7fzuanY3BSyE6ywzZqhkvk2D2HqZ01rO9jp1yz/XI75w4Ut4TTbu\\\\nnJQl8n/rIxYWL0954QaU63olaolVE6CuunIAf3+zc67adj1SEvzY33J9X9QQAtxy\\\\ntxW97+bXAgMBAAECggEAHNPJIcMtGMVc/aNuyBBvo7GetsUUFFgQ9Wwti6LkcdMd\\\\ngbft3TRVuiEDcdpXtjF2go6G1uQtztGb/WeL3Ldi076A5M8RYIpg95++Xn1ARvYy\\\\nIt6pbqlB47OC9iueU8E3EDpioc6WbPcV0Rb4WtWDnhQBCcx+ijs89rRHkpsGA6k1\\\\nWJcU6edtaUjjg0VNSdPgoLt7kVO5rG4e+8/JZHNWhtTej2EmDttwNAWbkXc9m02x\\\\nh08VOjBkPOWN0u77w/B+D4o3EaSy+J50eKvucQZL7/FC25RQsQMx8j3YlaW16SI6\\\\nhrl8y3GQFMBxB1x61Gzmghabw05aZMxaXCW0I4or/QKBgQDjvFokCrLgAamIk0EX\\\\nbbxHnSSaMYuGuJD1p858nlEbLqkbOBoscfSriv9UvU3z8SChnIyxRR0QG0uNAOU7\\\\nAUWU2lHwylkcWaWuP7DGDekuJarEtczNdgkJW5FCs49Y3oguiUmOrvPyvKsvO9+j\\\\n2U1X+h33lhrpy8WEQ/BV9OrsNQKBgQDgIOEXu1K0cq/pvuMt9NbHj3KnB10JBi9K\\\\nOv0M5Hyh6DRZWaTqHJcaIVKKti4HdwQmZFKAqrPW9bw88Jzmd2tk0szzaSDlEik6\\\\nD226GjtbI8ttffa6xNtF2PmG+6z5n2d7XD6iv+nhRpPSOrupZp+2wXdV+TEepgVq\\\\nN3J2NhQwWwKBgCq/mnQVhgpChZ5bdeZ99M/EkbBYCIbbcmqKe0otJbBii9/OIiuh\\\\noHnq947ikhVud9+eUVKdVJ1HYmEvf9gZnwQYDzWR5jRKJ62Tyw0CY3kEHRAbyDvx\\\\nIjUdeSD6L7hQ3WNO5f3gnW49OCz/ojVRfxE6pBFrndWxKHxxfFXGSarZAoGBAIiX\\\\n4Tb6ZhQtVUKKoJ/QeNlUdlaPaUbnqwh9XN1vnvl5r/XIaXZ4iCpXppt9Hh5UjYXs\\\\nXE+oOURHrToilizrGNtwkZHDP1X8NH37Cy+Bby1DamZHf4giE1TFQIXgenIDkSig\\\\ndlfvyFUQE2TrgJVkQvA5hkYyQZphKimqsyqSGMnDAoGBAKvNDuf6bXtmxV/j9SOD\\\\nXO5iTnkn4IKeuuCJBiUbxvpt08sUUTZrj+Z1OQ77Mz4sMge+p49gmgkeRtt+x/+3\\\\n3Te04b6ofJeiyMOYa29o6AhhRc/Dt5MdAZaJsP/O4IvqX6CvNIWNajPi3nsHQi7x\\\\n4n/rev4uFjv95q5ouWBJe0qp\\\\n-----END PRIVATE KEY-----\",\n",
        "  \"client_email\": \"tlearn-bot@tlearn-automation.iam.gserviceaccount.com\",\n",
        "  \"client_id\": \"115610121123746784958\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/tlearn-bot%40tlearn-automation.iam.gserviceaccount.com\",\n",
        "  \"universe_domain\": \"googleapis.com\"\n",
        "}'''\n",
        "# Modified client_secret to wrap it in \"installed\" key\n",
        "client_secret = '''{\n",
        "  \"installed\": {\n",
        "    \"client_id\": \"1023556486686-uv0lh742l79uplg2aa5h53e1kpk18m61.apps.googleusercontent.com\",\n",
        "    \"project_id\": \"tlearn-youtube-upload\",\n",
        "    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "    \"client_secret\": \"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\",\n",
        "    \"redirect_uris\": [\"http://localhost\"]\n",
        "  }\n",
        "}'''\n",
        "\n",
        "# Clean the service_account string by removing invalid control characters\n",
        "import re\n",
        "# More aggressive cleaning: remove all non-ASCII characters\n",
        "service_account_cleaned = re.sub(r'[^\\x00-\\x7F]+', '', service_account)\n",
        "\n",
        "\n",
        "# Replace escaped newlines with actual newlines in the service account private key\n",
        "service_account_cleaned = service_account_cleaned.replace('\\\\n', '\\n')\n",
        "\n",
        "# Print the cleaned string with escaped non-ASCII characters for debugging\n",
        "print(ascii(service_account_cleaned))\n",
        "\n",
        "\n",
        "with open(\"/content/your_service_account_credentials.json\", \"w\") as f:\n",
        "    f.write(service_account_cleaned)\n",
        "with open(\"/content/client_secret.json\", \"w\") as f:\n",
        "    f.write(client_secret)\n",
        "\n",
        "# üîó Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xaFie94IZDYJ"
      },
      "id": "xaFie94IZDYJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Bark + Replicate Short Video Generator (Step 2)\n",
        "\n",
        "!pip install replicate soundfile git+https://github.com/suno-ai/bark.git --quiet\n",
        "!sudo apt install -y ffmpeg\n",
        "\n",
        "import replicate, soundfile as sf, os, json, datetime\n",
        "from bark import generate_audio, SAMPLE_RATE\n",
        "\n",
        "# Set API key for Replicate\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_DRGD77kVGnpu3r3byCGSZkXkkZdt1hW3Tvncs\"\n",
        "\n",
        "# Load the metadata from Step 1\n",
        "today = datetime.date.today().isoformat()\n",
        "metadata_path = \"/content/drive/MyDrive/Tlearn_Backups/github_dashboard_data.json\"\n",
        "\n",
        "# Check if the metadata file exists, otherwise initialize with an empty list\n",
        "if os.path.exists(metadata_path):\n",
        "    with open(metadata_path, \"r\") as f:\n",
        "        concepts = json.load(f)\n",
        "else:\n",
        "    concepts = []\n",
        "    print(f\"Metadata file not found at {metadata_path}. Initializing with an empty list.\")\n",
        "\n",
        "\n",
        "# Output paths\n",
        "AUDIO_DIR = \"/content/drive/MyDrive/Tlearn_Backups/audio/\"\n",
        "VIDEO_DIR = \"/content/drive/MyDrive/Tlearn_Backups/videos/\"\n",
        "os.makedirs(AUDIO_DIR, exist_ok=True)\n",
        "os.makedirs(VIDEO_DIR, exist_ok=True)\n",
        "\n",
        "# Helper: Extract short sentence for video\n",
        "def get_short(script):\n",
        "    return ' '.join(script.strip().split(\"\\n\")[:2])  # first 2 lines of script\n",
        "\n",
        "# Helper: Generate Bark voiceover\n",
        "def generate_bark_audio(script, concept):\n",
        "    file_path = f\"{AUDIO_DIR}{today}_{concept}.wav\"\n",
        "    audio_array = generate_audio(script, history_prompt=\"v2/en_speaker_6\")\n",
        "    sf.write(file_path, audio_array, SAMPLE_RATE)\n",
        "    return file_path\n",
        "\n",
        "# Helper: Generate video using Pika or AnimateDiff\n",
        "def generate_video(prompt, model, out_path):\n",
        "    input_dict = {\"prompt\": prompt}\n",
        "    if model == \"pika\":\n",
        "        url = replicate.run(\"pika/pika\", input=input_dict)[\"video\"]\n",
        "    elif model == \"animatediff\":\n",
        "        url = replicate.run(\"cjwbw/animatediff\", input=input_dict)[\"video\"]\n",
        "    else:\n",
        "        return None\n",
        "    !wget \"{url}\" -O \"{out_path}\"\n",
        "    return url\n",
        "\n",
        "# Assign tools in round-robin: pika, animatediff, steve\n",
        "tools = [\"pika\", \"animatediff\", \"steve\"]\n",
        "\n",
        "for i, entry in enumerate(concepts):\n",
        "    if entry.get(\"status\") in [\"uploaded\", \"video_ready\"]:\n",
        "        continue  # Skip already done\n",
        "\n",
        "    concept = entry[\"concept\"]\n",
        "    script = entry[\"script\"]\n",
        "    short_text = get_short(script)\n",
        "\n",
        "    # üé§ Generate voice\n",
        "    audio_file = generate_bark_audio(script, concept)\n",
        "    entry[\"audio_path\"] = audio_file\n",
        "\n",
        "    # üéûÔ∏è Generate video\n",
        "    out_file = f\"{VIDEO_DIR}{today}_{concept}_short.mp4\"\n",
        "    tool = tools[i % len(tools)]\n",
        "\n",
        "    if tool in [\"pika\", \"animatediff\"]:\n",
        "        try:\n",
        "            video_url = generate_video(short_text, tool, out_file)\n",
        "            entry[\"short_link\"] = video_url\n",
        "            entry[\"status\"] = \"video_ready\"\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed on {concept} using {tool}: {e}\")\n",
        "            entry[\"status\"] = \"video_failed\"\n",
        "    else:\n",
        "        print(f\"üìé Manual needed for Steve.AI ‚Üí Audio: {audio_file}\")\n",
        "        entry[\"status\"] = \"manual_needed\"\n",
        "        entry[\"short_link\"] = audio_file\n",
        "\n",
        "# Save updated metadata\n",
        "with open(metadata_path, \"w\") as f:\n",
        "    json.dump(concepts, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Step 2 complete: Bark + Short videos ready.\")"
      ],
      "metadata": {
        "id": "AgdL95WVvyfm"
      },
      "id": "AgdL95WVvyfm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is redundant and has errors. The logic is handled in cell AgdL95WVvyfm.\n",
        "# import replicate, soundfile as sf\n",
        "# from bark import generate_audio, SAMPLE_RATE\n",
        "\n",
        "# os.environ[\"REPLICATE_API_TOKEN\"] = \"your_replicate_key_here\"\n",
        "\n",
        "# audio_dir = f\"{backup_dir}/audio/\"\n",
        "# video_dir = f\"{backup_dir}/videos/\"\n",
        "# os.makedirs(audio_dir, exist_ok=True)\n",
        "# os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "# def get_short(script): return ' '.join(script.split(\"\\n\")[:3])\n",
        "# def bark_voice(concept, script):\n",
        "#     path = f\"{audio_dir}{today}_{concept}.wav\"\n",
        "#     arr = generate_audio(script, history_prompt=\"v2/en_speaker_6\")\n",
        "#     sf.write(path, arr, SAMPLE_RATE)\n",
        "#     return path\n",
        "\n",
        "# tools = ['pika', 'animatediff', 'steve']\n",
        "\n",
        "# # This loop uses the undefined 'log' variable and duplicates logic from cell AgdL95WVvyfm\n",
        "# # for i, entry in enumerate(log):\n",
        "# #     c, s = entry['concept'], entry['script']\n",
        "# #     audio = bark_voice(c, s)\n",
        "# #     entry['audio_path'] = audio\n",
        "# #     short_text = get_short(s)\n",
        "# #     out_file = f\"{video_dir}{today}_{c}_short.mp4\"\n",
        "\n",
        "# #     if tools[i % 3] == \"pika\":\n",
        "# #         url = replicate.run(\"pika/pika\", input={\"prompt\": short_text})['video']\n",
        "# #         !wget \"{url}\" -O \"{out_file}\"\n",
        "# #         entry['short_link'] = url\n",
        "# #     elif tools[i % 3] == \"animatediff\":\n",
        "# #         url = replicate.run(\"cjwbw/animatediff\", input={\"prompt\": short_text})['video']\n",
        "# #         !wget \"{url}\" -O \"{out_file}\"\n",
        "# #         entry['short_link'] = url\n",
        "# #     else:\n",
        "# #         print(f\"üìù Manual upload: {audio}\")\n",
        "# #         entry['short_link'] = \"Manual\"\n",
        "\n",
        "# # entry['status'] = \"video_ready\"\n",
        "# # with open(dashboard_path, \"w\") as f: json.dump(log, f, indent=2)"
      ],
      "metadata": {
        "id": "x9pwoWXHthPh"
      },
      "id": "x9pwoWXHthPh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google_auth_oauthlib.flow import InstalledAppFlow\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "import os # Import os as it's used later\n",
        "\n",
        "def youtube_auth():\n",
        "    # This function is already defined and appears correct based on previous executions.\n",
        "    # Assuming the client_secret.json is correctly created in a previous cell.\n",
        "    flow = InstalledAppFlow.from_client_secrets_file(\"/content/client_secret.json\", scopes=[\"https://www.googleapis.com/auth/youtube.upload\"])\n",
        "    # Use run_local_server or run_flow instead of run_console for Colab\n",
        "    creds = flow.run_local_server(port=0) # Use run_local_server\n",
        "    return build(\"youtube\", \"v3\", credentials=creds)\n",
        "\n",
        "# Ensure yt is initialized - this will require user interaction\n",
        "# yt = youtube_auth()\n",
        "\n",
        "def ai_metadata(c, s):\n",
        "    # Ensure openai is imported and api_key is set in a separate cell\n",
        "    # This function assumes openai.ChatCompletion.create works correctly.\n",
        "    # Added a basic structure check for the response.\n",
        "    try:\n",
        "        r = openai.ChatCompletion.create(model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": f\"Create viral YouTube Shorts metadata for {c}\"}])\n",
        "        # Check if the expected structure exists\n",
        "        if r and 'choices' in r and len(r['choices']) > 0 and 'message' in r['choices'][0] and 'content' in r['choices'][0]['message']:\n",
        "            out = r['choices'][0]['message']['content']\n",
        "            title = f\"{c} - NEET 3D\"\n",
        "            # Ensure description is not too long for YouTube API\n",
        "            desc = out[:5000] # Truncate description if it exceeds limit\n",
        "            # Generate tags - ensuring they are in a list format and within length limits\n",
        "            # Simple tag generation based on concept for now. More sophisticated tag generation could be added.\n",
        "            tags = [\"NEET\", \"3D\", c.replace(\" \", \"_\")[:50]] # Basic tag based on concept, truncated\n",
        "            # Add more general NEET related tags\n",
        "            tags.extend([\"neet preparation\", \"biology neet\", \"chemistry neet\", \"physics neet\", \"neet shorts\"])\n",
        "            # Remove duplicates and limit total tags if necessary (YouTube limit is around 500 characters total)\n",
        "            tags = list(set(tags))\n",
        "            # Join tags into a string for the API if needed, though the API usually takes a list.\n",
        "            # If the API requires a comma-separated string: tags_string = \",\".join(tags)\n",
        "            return title, desc, tags\n",
        "        else:\n",
        "            print(f\"Warning: Unexpected response structure from OpenAI for concept {c}. Skipping metadata generation.\")\n",
        "            # Provide default metadata\n",
        "            title = f\"{c} - NEET 3D\"\n",
        "            desc = f\"A NEET 3D video about {c}\"\n",
        "            tags = [\"NEET\", \"3D\", \"shorts\"]\n",
        "            return title, desc, tags\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating AI metadata for {c}: {e}\")\n",
        "        # Provide default metadata in case of OpenAI error\n",
        "        title = f\"{c} - NEET 3D\"\n",
        "        desc = f\"A NEET 3D video about {c}\"\n",
        "        tags = [\"NEET\", \"3D\", \"shorts\"]\n",
        "        return title, desc, tags\n",
        "\n",
        "\n",
        "# --- Start of modified logic ---\n",
        "# Assuming 'concepts' variable holds the data (as populated by cell AgdL95WVvyfm)\n",
        "# Ensure concepts is defined and is a list before iterating\n",
        "if 'concepts' in globals() and isinstance(concepts, list):\n",
        "    # Assuming backup_dir, video_dir, and today are defined in previous cells\n",
        "    # Ensure backup_dir and video_dir exist\n",
        "    if 'backup_dir' not in globals() or not os.path.exists(backup_dir):\n",
        "        print(\"Error: backup_dir is not defined or does not exist. Please run previous cells.\")\n",
        "    elif 'video_dir' not in globals() or not os.path.exists(video_dir):\n",
        "        print(\"Error: video_dir is not defined or does not exist. Please run previous cells.\")\n",
        "    elif 'today' not in globals():\n",
        "         print(\"Error: today is not defined. Please run previous cells.\")\n",
        "    else:\n",
        "        # Initialize yt if it's not already\n",
        "        if 'yt' not in globals():\n",
        "            print(\"Initializing YouTube authentication. This may require user interaction.\")\n",
        "            try:\n",
        "                yt = youtube_auth()\n",
        "                print(\"YouTube authentication successful.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during YouTube authentication: {e}\")\n",
        "                yt = None # Set yt to None if auth fails\n",
        "\n",
        "        if yt: # Proceed only if YouTube authentication was successful\n",
        "            for entry in concepts: # Use 'concepts' instead of 'log'\n",
        "                # Check if the entry status is 'video_ready' before attempting upload\n",
        "                if entry.get(\"status\") == \"video_ready\":\n",
        "                    c = entry['concept']\n",
        "                    # Construct the expected file path based on the naming convention in cell AgdL95WVvyfm\n",
        "                    path = f\"{video_dir}{today}_{c}_short.mp4\"\n",
        "\n",
        "                    if os.path.exists(path):\n",
        "                        print(f\"Uploading video for concept: {c} from {path}\")\n",
        "                        try:\n",
        "                            title, desc, tags = ai_metadata(c, entry['script'])\n",
        "                            body = {\"snippet\": {\"title\": title, \"description\": desc, \"tags\": tags}, \"status\": {\"privacyStatus\": \"public\"}}\n",
        "\n",
        "                            # Upload the video\n",
        "                            # Ensure MediaFileUpload is correctly used\n",
        "                            media_body = MediaFileUpload(path, resumable=True) # Use resumable upload\n",
        "                            request = yt.videos().insert(\n",
        "                                part=\"snippet,status\",\n",
        "                                body=body,\n",
        "                                media_body=media_body\n",
        "                            )\n",
        "                            res = request.execute()\n",
        "\n",
        "                            # Update the entry with the uploaded video details\n",
        "                            entry.update({\"short_link\": f\"https://youtu.be/{res['id']}\", \"status\": \"uploaded\"})\n",
        "                            print(f\"Successfully uploaded video for {c}. YouTube URL: {entry['short_link']}\")\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"‚ùå Error uploading video for {c}: {e}\")\n",
        "                            entry[\"status\"] = \"upload_failed\"\n",
        "                            entry[\"short_link\"] = \"Upload Failed\" # Indicate failure\n",
        "\n",
        "                    else:\n",
        "                        print(f\"‚ùó Video file not found for concept: {c} at {path}. Skipping upload.\")\n",
        "                        entry[\"status\"] = \"video_file_missing\" # Indicate file missing\n",
        "\n",
        "                elif entry.get(\"status\") in [\"uploaded\", \"upload_failed\", \"video_file_missing\", \"manual_needed\"]:\n",
        "                    print(f\"Skipping upload for concept {entry.get('concept', 'N/A')} with status: {entry.get('status', 'N/A')}\")\n",
        "                else:\n",
        "                     print(f\"‚ùó Skipping upload for concept {entry.get('concept', 'N/A')} with unexpected status: {entry.get('status', 'N/A')}. Entry: {entry}\")\n",
        "\n",
        "\n",
        "            # Save updated metadata back to the dashboard file\n",
        "            # Assuming dashboard_path is defined\n",
        "            if 'dashboard_path' in globals():\n",
        "                with open(dashboard_path, \"w\") as f:\n",
        "                    json.dump(concepts, f, indent=2)\n",
        "                print(f\"Updated metadata saved to {dashboard_path}\")\n",
        "            else:\n",
        "                print(\"Error: dashboard_path is not defined. Could not save updated metadata.\")\n",
        "\n",
        "        else:\n",
        "            print(\"YouTube authentication failed. Skipping video uploads.\")\n",
        "else:\n",
        "    print(\"Error: 'concepts' variable not found or is not a list. Please ensure cell AgdL95WVvyfm ran successfully.\")\n",
        "\n",
        "# --- End of modified logic ---"
      ],
      "metadata": {
        "id": "yLZVLuW0Zwb6"
      },
      "id": "yLZVLuW0Zwb6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from telegram import Bot\n",
        "bot = Bot(token=\"7992613988:AAGmu39UGP9zGQoZwrhHTshV8fl6DFMQLu0\")\n",
        "for entry in concepts:\n",
        "    if entry.get('status') == \"uploaded\":\n",
        "        # Assuming you have a channel name to send messages to. Replace 'your_channel_name' with the actual channel username.\n",
        "        # Also, ensure the bot has admin rights in the channel to send messages.\n",
        "        msg = f\"üé¨ New NEET 3D Video: {entry['concept']}\\n{entry['short_link']}\"\n",
        "        # Replace '@your_channel_name' with your actual Telegram channel username\n",
        "        try:\n",
        "            bot.send_message(chat_id=\"@your_channel_name\", text=msg)\n",
        "            print(f\"Sent Telegram message for concept: {entry['concept']}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error sending Telegram message for {entry['concept']}: {e}\")"
      ],
      "metadata": {
        "id": "V-JcRvP2Z2Gn"
      },
      "id": "V-JcRvP2Z2Gn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2d464aa1",
      "metadata": {
        "id": "2d464aa1"
      },
      "source": [
        "# üìò Tlearn Full Pipeline (Steps 1‚Äì4)\n",
        "Includes OpenAI, Replicate, Telegram, and YouTube API integrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52e6a110",
      "metadata": {
        "id": "52e6a110"
      },
      "outputs": [],
      "source": [
        "# OpenAI Key\n",
        "import openai\n",
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef74dc64",
      "metadata": {
        "id": "ef74dc64"
      },
      "outputs": [],
      "source": [
        "# Replicate Key\n",
        "import os\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_DRGD77kVGnpu3r3byCGSZkXkkZdt1hW3Tvncs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ebe292a",
      "metadata": {
        "id": "9ebe292a"
      },
      "outputs": [],
      "source": [
        "# Telegram Token\n",
        "from telegram import Bot\n",
        "bot = Bot(token=\"7992613988:AAGmu39UGP9zGQoZwrhHTshV8fl6DFMQLu0\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d251420d"
      },
      "source": [
        "!pip install python-telegram-bot"
      ],
      "id": "d251420d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70f91ee3"
      },
      "source": [
        "!pip install gspread"
      ],
      "id": "70f91ee3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bd943b2"
      },
      "source": [
        "!pip install oauth2client"
      ],
      "id": "5bd943b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddfd5f17"
      },
      "source": [
        "!pip install pytrends"
      ],
      "id": "ddfd5f17",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98a56aa9"
      },
      "source": [
        "!pip install replicate"
      ],
      "id": "98a56aa9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86cf1075"
      },
      "source": [
        "!pip install soundfile"
      ],
      "id": "86cf1075",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aa8dcee"
      },
      "source": [
        "!pip install git+https://github.com/suno-ai/bark.git"
      ],
      "id": "9aa8dcee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a070dbbc"
      },
      "source": [
        "service_account_lines = service_account.splitlines()\n",
        "for i, line in enumerate(service_account_lines):\n",
        "    print(f\"Line {i+1}: {line}\")"
      ],
      "id": "a070dbbc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a1d5f13"
      },
      "source": [
        "with open(\"/content/your_service_account_credentials.json\", \"rb\") as f:\n",
        "    file_content_bytes = f.read()\n",
        "\n",
        "# The error is at line 5, column 46 in the JSON structure.\n",
        "# We need to find the corresponding byte offset in the file content.\n",
        "# This is tricky because newlines and character encoding affect the position.\n",
        "# Let's print the content and manually find the position or print a section around the estimated position.\n",
        "\n",
        "print(\"File content as bytes (first 500 bytes):\")\n",
        "print(file_content_bytes[:500])\n",
        "\n",
        "# To find the exact byte at line 5, column 46, you would need to\n",
        "# manually count characters and newlines in the file content or use a\n",
        "# text editor that shows byte offsets.\n",
        "# However, we know the error is at char 177. Let's look at the byte at that offset.\n",
        "# NOTE: This assumes a 1-byte-per-character encoding like ASCII for the error position,\n",
        "# which might not be entirely accurate depending on the actual character and encoding.\n",
        "\n",
        "char_index_from_error = 46 -1 # Column is 1-based\n",
        "# We need to find the start of line 5 in the byte content.\n",
        "# This requires parsing or manual inspection.\n",
        "\n",
        "# Let's try printing around the estimated location based on the error message\n",
        "# assuming roughly ASCII characters for character count.\n",
        "estimated_byte_offset = 0\n",
        "line_num = 1\n",
        "col_num = 1\n",
        "for i, b in enumerate(file_content_bytes):\n",
        "    if line_num == 5 and col_num == 46:\n",
        "        estimated_byte_offset = i\n",
        "        break\n",
        "    if b == ord(b'\\n'): # Check for newline byte\n",
        "        line_num += 1\n",
        "        col_num = 1\n",
        "    else:\n",
        "        col_num += 1\n",
        "\n",
        "print(f\"\\nEstimated byte offset for line 5, column 46: {estimated_byte_offset}\")\n",
        "\n",
        "# Print a few bytes before and after the estimated error location\n",
        "start = max(0, estimated_byte_offset - 20)\n",
        "end = min(len(file_content_bytes), estimated_byte_offset + 20)\n",
        "print(f\"Bytes around estimated error location ({start}-{end}):\")\n",
        "print(file_content_bytes[start:end])\n",
        "\n",
        "# The byte at the error location is char 177 (0xb1).\n",
        "# Let's see if this byte is present near the estimated location.\n",
        "print(f\"\\nByte value at estimated error offset: {file_content_bytes[estimated_byte_offset] if estimated_byte_offset < len(file_content_bytes) else 'Offset out of bounds'}\")"
      ],
      "id": "3a1d5f13",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26a17207"
      },
      "source": [
        "# Task\n",
        "Fix the `JSONDecodeError` in the provided code snippet by ensuring the `service_account` string contains a valid and correctly formatted JSON key, and then execute the entire code to fix any other errors."
      ],
      "id": "26a17207"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18fcec09"
      },
      "source": [
        "## Fix the credential loading error\n",
        "\n",
        "### Subtask:\n",
        "Fix the `JSONDecodeError` in the provided code snippet by ensuring the `service_account` string contains a valid and correctly formatted JSON key.\n"
      ],
      "id": "18fcec09"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da6ab3e2"
      },
      "source": [
        "**Reasoning**:\n",
        "The error message and traceback point to an invalid control character at a specific location in the `service_account` string. The previous execution of cell `3a1d5f13` also showed that byte `177` (decimal) corresponds to character `177` (octal `\\261`) which is a non-ASCII character (specifically `¬±` or plus-minus sign in some encodings), confirming the issue is a stray non-ASCII character within the JSON string. This character needs to be removed or replaced with the correct character to make the JSON valid. Looking at the string, the problematic character seems to be in the `private_key_id` field. Based on the context, `d69ba204f9ffb36f8a664855d56f6f2d8ee912b9` appears to be a valid private key ID. The stray character is likely a copy-paste error. I will remove the identified invalid character and ensure the private key field has correctly formatted newlines.\n",
        "\n"
      ],
      "id": "da6ab3e2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e95bff69"
      },
      "source": [
        "# Update the service_account string to fix the JSONDecodeError\n",
        "service_account = '''{\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"tlearn-automation\",\n",
        "  \"private_key_id\": \"d69ba204f9ffb36f8a664855d56f6f2d8ee912b9\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDHYg6odafJtUqb\\\\njG6zxvxsev4bF60mb4jXTXGIu0jKLwTDlpF2n6s7qP5cor0pKkmDj3wWSiBrnsKv\\\\naL0gXA/O+RZdc29yv5CHGruxlhY9o38Mputhlzc8N+Ofnm2JhJTLejwePO4CES1M\\\\nfjVoS7bF1ekcehJ45TDb2CFc6gnTYXkhLWMp+wZWSZvucZviNRs2lxLiB5BGsff9\\\\n4w+d0DyyBWke7fzuanY3BSyE6ywzZqhkvk2D2HqZ01rO9jp1yz/XI75w4Ut4TTbu\\\\nnJQl8n/rIxYWL0954QaU63olaolVE6CuunIAf3+zc67adj1SEvzY33J9X9QQAtxy\\\\ntxW97+bXAgMBAAECggEAHNPJIcMtGMVc/aNuyBBvo7GetsUUFFgQ9Wwti6LkcdMd\\\\ngbft3TRVuiEDcdpXtjF2go6G1uQtztGb/WeL3Ldi076A5M8RYIpg95++Xn1ARvYy\\\\nIt6pbqlB47OC9iueU8E3EDpioc6WbPcV0Rb4WtWDnhQBCcx+ijs89rRHkpsGA6k1\\\\nWJcU6edtaUjjg0VNSdPgoLt7kVO5rG4e+8/JZHNWhtTej2EmDttwNAWbkXc9m02x\\\\nh08VOjBkPOWN0u77w/B+D4o3EaSy+J50eKvucZZL7/FC25RQsQMx8j3YlaW16SI6\\\\nhrl8y3GQFMBxB1x61Gzmghabw05aZMxaXCW0I4or/QKBgQDjvFokCrLgAamIk0EX\\\\nbbxHnSSaMYuGuJD1p858nlEbLqkbOBoscfSriv9UvU3z8SChnIyxRR0QG0uNAOU7\\\\nAUWU2lHwylkcWaWuP7DGDekuJarEtczNdgkJW5FCs49Y3oguiUmOrvPyvKsvO9+j\\\\n2U1X+h33lhrpy8WEQ/BV9OrsNQKBgQDgIOEXu1K0cq/pvuMt9NbHj3KnB10JBi9K\\\\nOv0M5Hyh6DRZWaTqHJcaIVKKti4HdwQmZFKAqrPW9bw88Jzmd2tk0szzaSDlEik6\\\\nD226GjtbI8ttffa6xNtF2PmG+6z5n2d7XD6iv+nhRpPSOrupZp+2wXdV+TEepgVq\\\\nN3J2NhQwWwKBgCq/mnQVhgpChZ5bdeZ99M/EkbBYCIbbcmqKe0otJbBii9/OIiuh\\\\noHnq947ikhVud9+eUVKdVJ1HYmEvf9gZnwQYDzWR5jRKJ62Tyw0CY3kEHRAbyDvx\\\\nIjUdeSD6L7hQ3WNO5f3gnW49OCz/ojVRfxE6pBFrndWxKHxxfFXGSarZAoGBAIiX\\\\n4Tb6ZhQtVUKKoJ/QeNlUdlaPaUbnqwh9XN1vnvl5r/XIaXZ4iCpXppt9Hh5UjYXs\\\\nXE+oOURHrToilizrGNtwkZHDP1X8NH37Cy+Bby1DamZHf4giE1TFQIXgenIDkSig\\\\ndlfvyFUQE2TrgJVkQvA5hkYyQZphKimqsyqSGMnDAoGBAKvNDuf6bXtmxV/j9SOD\\\\nXO5iTnkn4IKeuuCJBiUbxvpt08sUUTZrj+Z1OQ77Mz4sMge+p49gmgkeRtt+x/+3\\\\n3Te04b6ofJeiyMOYa29o6AhhRc/Dt5MdAZaJsP/O4IvqX6CvNIWNajPi3nsHQi7x\\\\n4n/rev4uFjv95q5ouWBJe0qp\\\\n-----END PRIVATE KEY-----\",\n",
        "  \"client_email\": \"tlearn-bot@tlearn-automation.iam.gserviceaccount.com\",\n",
        "  \"client_id\": \"115610121123746784958\",\n",
        "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/tlearn-bot%40tlearn-automation.iam.gserviceaccount.com\",\n",
        "  \"universe_domain\": \"googleapis.com\"\n",
        "}'''\n",
        "\n",
        "# The previous cleaning attempt was too aggressive. Let's manually fix the problematic character.\n",
        "# The error is at line 5, column 46. Looking at the string, this corresponds to a character\n",
        "# right before the \"d\" in \"d69ba204f9ffb36f8a664855d56f6f2d8ee912b9\".\n",
        "# Based on the byte value 177 (0xb1), this is likely a non-ASCII character that was\n",
        "# accidentally included during copy-pasting. I will remove the character at that position.\n",
        "\n",
        "# Find the index of the start of the private_key_id value\n",
        "start_index = service_account.find('\"private_key_id\": \"') + len('\"private_key_id\": \"')\n",
        "# The error is at line 5, column 46. Let's count characters to find the exact index.\n",
        "# Line 1: {\"type\": \"service_account\",\\n (31 chars + newline) = 32\n",
        "# Line 2: \"project_id\": \"tlearn-automation\",\\n (33 + newline) = 34\n",
        "# Line 3: \"private_key_id\": \" (18)\n",
        "# Line 4: d69ba204f9ffb36f8a664855d56f6f2d8ee912b9\",\\n (40 + newline) = 41\n",
        "# Line 5: \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n (36 + newline) = 37\n",
        "# The error is reported at line 5, column 46 (char 177 overall based on previous output).\n",
        "# Let's re-examine the raw service_account string and the error traceback.\n",
        "# The error is at line 5, column 46. In the provided string, line 5 starts with `\"private_key\": \"`.\n",
        "# Counting from the start of line 5:\n",
        "# \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n\n",
        "# 1234567890123456789012345678901234567890123456\n",
        "# The character at column 46 is the first character after `-----BEGIN PRIVATE KEY-----\\n`.\n",
        "# Looking at the original string, there seems to be a character issue in the `private_key_id` field itself,\n",
        "# which was reported at line 5 in the *previous* attempt, but the string has been modified since.\n",
        "# Let's go back to the original error message: `Invalid control character at: line 5 column 46 (char 177)`.\n",
        "# This error happened *before* the aggressive cleaning.\n",
        "# Let's look at the original string in cell xaFie94IZDYJ again.\n",
        "# service_account = '''{... \"private_key_id\": \"d69ba204f9ffb36f8a664855d56f6f2d8ee912b9\", ...}'''\n",
        "# Line 5 would be the line containing the private key. Let's re-count based on the original string structure.\n",
        "# Line 1: {\n",
        "# Line 2:   \"type\": \"service_account\",\n",
        "# Line 3:   \"project_id\": \"tlearn-automation\",\n",
        "# Line 4:   \"private_key_id\": \"d69ba204f9ffb36f8a664855d56f6f2d8ee912b9\",\n",
        "# Line 5:   \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n\n",
        "# The error is at line 5, column 46. This confirms the issue is within the `private_key` value, specifically at the beginning of the key content after the `-----BEGIN PRIVATE KEY-----\\n`.\n",
        "# The error message `(char 177)` likely refers to the byte offset in the file.\n",
        "# Let's look at the private_key content in the original string again.\n",
        "# It starts with `-----BEGIN PRIVATE KEY-----\\\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDHYg6odafJtUqb\\\\njG6zxvxsev4bF60mb4jXTXGIu0jKLwTDlpF2n6s7qP5cor0pKkmDj3wWSiBrnsKv\\\\naL0gXA/O+RZdc29yv5CHGruxlhY9o38Mputhlzc8N+Ofnm2JhJTLejwePO4CES1M\\\\nfjVoS7bF1ekcehJ45TDb2CFc6gnTYXkhLWMp+wZWSZvucZviNRs2lxLiB5BGsff9\\\\n4w+d0DyyBWke7fzuanY3BSyE6ywzZqhkvk2D2HqZ01rO9jp1yz/XI75w4Ut4TTbu\\\\nnJQl8n/rIxYWL0954QaU63olaolVE6CuunIAf3+zc67adj1SEvzY33J9X9QQAtxy\\\\ntxW97+bXAgMBAAECggEAHNPJIcMtGMVc/aNuyBBvo7GetsUUFFgQ9Wwti6LkcdMd\\\\ngbft3TRVuiEDcdpXtjF2go6G1uQtztGb/WeL3Ldi076A5M8RYIpg95++Xn1ARvYy\\\\nIt6pbqlB47OC9iueU8E3EDpioc6WbPcV0Rb4WtWDnhQBCcx+ijs89rRHkpsGA6k1\\\\nWJcU6edtaUjjg0VNSdPgoLt7kVO5rG4e+8/JZHNWhtTej2EmDttwNAWbkXc9m02x\\\\nh08VOjBkPOWN0u77w/B+D4o3EaSy+J50eKvucGZL7/FC25RQsQMx8j3YlaW16SI6\\\\nhrl8y3GQFMBxB1x61Gzmghabw05aZMxaXCW0I4or/QKBgQDjvFokCrLgAamIk0EX\\\\nbbxHnSSaMYuGuJD1p858nlEbLqkbOBoscfSriv9UvU3z8SChnIyxRR0QG0uNAOU7\\\\nAUWU2lHwylkcWaWuP7DGDekuJarEtczNdgkJW5FCs49Y3oguiUmOrvPyvKsvO9+j\\\\n2U1X+h33lhrpy8WEQ/BV9OrsNQKBgQDgIOEXu1K0cq/pvuMt9NbHj3KnB10JBi9K\\\\nOv0M5Hyh6DRZWaTqHJcaIVKKti4HdwQmZFKAqrPW9bw88Jzmd2tk0szzaSDlEik6\\\\nD226GjtbI8ttffa6xNtF2PmG+6z5n2d7XD6iv+nhRpPSOrupZp+2wXdV+TEepgVq\\\\nN3J2NhQwWwKBgCq/mnQVhgpChZ5bdeZ99M/EkbBYCIbbcmqKe0otJbBii9/OIiuh\\\\noHnq947ikhVud9+eUVKdVJ1HYmEvf9gZnwQYDzWR5jRKJ62Tyw0CY3kEHRAbyDvx\\\\nIjUdeSD6L7hQ3WNO5f3gnW49OCz/ojVRfxE6pBFrndWxKHxxfFXGSarZAoGBAIiX\\\\n4Tb6ZhQtVUKKoJ/QeNlUdlaPaUbnqwh9XN1vnvl5r/XIaXZ4iCpXppt9Hh5UjYXs\\\\nXE+oOURHrToilizrGNtwkZHDP1X8NH37Cy+Bby1DamZHf4giE1TFQIXgenIDkSig\\\\ndlfvyFUQE2TrgJVkQvA5hkYyQZphKimqsyqSGMnDAoGBAKvNDuf6bXtmxV/j9SOD\\\\nXO5iTnkn4IKeuuCJBiUbxvpt08sUUTZrj+Z1OQ77Mz4sMge+p49gmgkeRtt+x/+3\\\\n3Te04b6ofJeiyMOYa29o6AhhRc/Dt5MdAZaJsP/O4IvqX6CvNIWNajPi3nsHQi7x\\\\n4n/rev4uFjv95q5ouWBJe0qp\\\\n-----END PRIVATE KEY-----\"'''\n",
        "# Let's carefully examine the string. The traceback said line 5 column 46.\n",
        "# Counting characters in the raw string on line 5:\n",
        "#   \"private_key\": \"-----BEGIN PRIVATE KEY-----\\\\n\n",
        "#  1234567890123456789012345678901234567890123456\n",
        "# The character at column 46 is the backslash before the 'n' in '\\\\nMIIEvg'.\n",
        "# This backslash is correctly escaped for JSON. The issue might be a hidden character before or after this point.\n",
        "# The error also mentions `(char 177)`. Let's count the characters/bytes up to line 5 column 46.\n",
        "# Line 1: {\"type\": \"service_account\",\\n (32 bytes including newline if UTF-8)\n",
        "# Line 2:   \"project_id\": \"tlearn-automation\",\\n (34 bytes)\n",
        "# Line 3:   \"private_key_id\": \"d69ba204f9ffb36f8a664855d56f6f2d8ee912b9\",\\n (60 bytes)\n",
        "# Line 4:   \"private_key\": \" (17 bytes)\n",
        "# Line 5: \"-----BEGIN PRIVATE KEY-----\\\\n (36 bytes)\n",
        "# Total bytes: 32 + 34 + 60 + 17 + 36 = 179 bytes. This is close to 177.\n",
        "# Let's re-examine the output from the previous cell which printed the bytes around offset 177.\n",
        "# `file_content_bytes[start:end]` showed bytes including `b'\\\\xf0\\\\x9d\\\\x97\\\\xab\\\\xef\\\\xb8\\\\x8f'` before the private key content.\n",
        "# This sequence `\\\\xf0\\\\x9d\\\\x97\\\\xab\\\\xef\\\\xb8\\\\x8f` is a UTF-8 encoded character (likely an emoji or symbol) that is invalid within a JSON string unless properly escaped.\n",
        "# This character is likely located just before `-----BEGIN PRIVATE KEY-----`.\n",
        "# I will manually remove this sequence from the `service_account` string.\n",
        "\n",
        "# Locate the position of the problematic character sequence based on the byte output.\n",
        "# The byte output was `...b'\": \"\\\\xf0\\\\x9d\\\\x97\\\\xab\\\\xef\\\\xb8\\\\x8f-----BEGIN PRIVATE KEY-----...`\n",
        "# The sequence `\\\\xf0\\\\x9d\\\\x97\\\\xab\\\\xef\\\\xb8\\\\x8f` is just after `\": \"`.\n",
        "# I will remove `\\\\xf0\\\\x9d\\\\x97\\\\xab\\\\xef\\\\xb8\\\\x8f` from the string.\n",
        "\n",
        "# I will redefine the service_account string, carefully removing the identified invalid characters.\n",
        "# I will also ensure the private key newlines are correctly represented as \\\\n in the string literal for JSON.\n",
        "service_account = '''{\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"tlearn-automation\",\n",
        "  \"private_key_id\": \"d69ba204f9ffb36f8a664855d56f6f2d8ee912b9\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDHYg6odafJtUqb\\\\njG6zxvxsev4bF60mb4jXTXGIu0jKLwTDlpF2n6s7qP5cor0pKkmDj3wWSiBrnsKv\\\\naL0gXA/O+RZdc29yv5CHGruxlhY9o38Mputhlzc8N+Ofnm2JhJTLejwePO4CES1M\\\\nfjVoS7bF1ekcehJ45TDb2CFc6gnTYXkhLWMp+wZWSZvucZviNRs2lxLiB5BGsff9\\\\n4w+d0DyyBWke7fzuanY3BSyE6ywzZqhkvk2D2HqZ01rO9jp1yz/XI75w4Ut4TTbu\\\\nnJQl8n/rIxYWL0954QaU63olaolVE6CuunIAf3+zc67adj1SEvzY33J9X9QQAtxy\\\\ntxW97+bXAgMBAAECggEAHNPJIcMtGMVc/aNuyBBvo7GetsUUFFgQ9Wwti6LkcdMd\\\\ngbft3TRVuiEDcdpXtjF2go6G1uQtztGb/WeL3Ldi076A5M8RYIpg95++Xn1ARvYy\\\\nIt6pbqlB47OC9iueU8E3EDpioc6WbPcV0Rb4WtWDnhQBCcx+ijs89rRHkpsGA6k1\\\\nWJcU6edtaUjjg0VNSdPgoLt7kVO5rG4e+8/JZHNWhtTej2EmDttwNAWbkXc9m02x\\\\nh08VOjBkPOWN0u77w/B+D4o3EaSy+J50eKvucGZL7/FC25RQsQMx8j3YlaW16SI6\\\\nhrl8y3GQFMBxB1x61Gzmghabw05aZMxaXCW0I4or/QKBgQDjvFokCrLgAamIk0EX\\\\nbbxHnSSaMYuGuJD1p858nlEbLqkbOBoscfSriv9UvU3z8SChnIyxRR0QG0uNAOU7\\\\nAUWU2lHwylkcWaWuP7DGDekuJarEtczNdgkJW5FCs49Y3oguiUmOrvPyvKsvO9+j\\\\n2U1X+h33lhrpy8WEQ/BV9OrsNQKBgQDgIOEXu1K0cq/pvuMt9NbHj3KnB10JBi9K\\\\nOv0M5Hyh6DRZWaTqHJcaIVKKti4HdwQmZFKAqrPW9bw88Jzmd2tk0szzaSDlEik6\\\\nD226GjtbI8ttffa6xNtF2PmG+6z5n2d7XD6iv+nhRpPSOrupZp+2wXdV+TEepgVq\\\\nN3J2NhQwWwKBgCq/mnQVhgpChZ5bdeZ99M/EkbBYCIbbcmqKe0otJbBii9/OIiuh\\\\noHnq947ikhVud9+eUVKdVJ1HYmEvf9gZnwQYDzWR5jRKJ62Tyw0CY3kEHRAbyDvx\\\\nIjUdeSD6L7hQ3WNO5f3gnW49OCz/ojVRfxE6pBFrndWxKHxxfFXGSarZAoGBAIiX\\\\n4Tb6ZhQtVUKKoJ/QeNlUdlaPaUbnqwh9XN1vnvl5r/XIaXZ4iCpXppt9Hh5UjYXs\\\\nXE+oOURHrToilizrGNtwkZHDP1X8NH37Cy+Bby1DamZHf4giE1TFQIXgenIDkSig\\\\ndlfvyFUQE2TrgJVkQvA5hkYyQZphKimqsyqSGMnDAoGBAKvNDuf6bXtmxV/j9SOD\\\\nXO5iTnkn4IKeuuCJBiUbxvpt08sUUTZrj+Z1OQ77Mz4sMge+p49gmgkeRtt+x/+3\\\\n3Te04b6ofJeiyMOYa29o6AhhRc/Dt5MdAZaJsP/O4IvqX6CvNIWNajPi3nsHQi7x\\\\n4n/rev4uFjv95q5ouWBJe0qp\\\\n-----END PRIVATE KEY-----\"\n",
        "}'''\n",
        "\n",
        "# Replace escaped newlines with actual newlines for the file\n",
        "service_account_for_file = service_account.replace('\\\\n', '\\n')\n",
        "\n",
        "with open(\"/content/your_service_account_credentials.json\", \"w\") as f:\n",
        "    f.write(service_account_for_file)\n",
        "\n",
        "# Verify the content written to the file (optional, for debugging)\n",
        "# with open(\"/content/your_service_account_credentials.json\", \"r\") as f:\n",
        "#     print(\"\\nContent written to file:\")\n",
        "#     print(f.read())\n"
      ],
      "id": "e95bff69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a151a0b7"
      },
      "source": [
        "**Reasoning**:\n",
        "I have updated the `service_account` variable and written the corrected content to the file. Now I need to re-execute the cell that attempts to load the credentials using `gspread` and `oauth2client` to confirm that the `JSONDecodeError` is resolved.\n",
        "\n"
      ],
      "id": "a151a0b7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f8febdd"
      },
      "source": [
        "import gspread, datetime, os, json, pandas as pd\n",
        "from pytrends.request import TrendReq\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "# openai.api_key is set in a separate cell\n",
        "\n",
        "sheet_name = \"Tlearn Concepts Log\"\n",
        "backup_dir = \"/content/drive/MyDrive/Tlearn_Backups/\"\n",
        "os.makedirs(backup_dir, exist_ok=True)\n",
        "dashboard_path = backup_dir + \"github_dashboard_data.json\"\n",
        "\n",
        "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
        "# Load credentials from file\n",
        "try:\n",
        "    creds = ServiceAccountCredentials.from_json_keyfile_name(\"/content/your_service_account_credentials.json\", scope)\n",
        "    print(\"Credentials loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading credentials: {e}\")\n",
        "    # This part of the error handling is no longer needed as the JSON is fixed.\n",
        "    # Keeping it for now to see if any other error occurs.\n",
        "    try:\n",
        "        with open(\"/content/your_service_account_credentials.json\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
        "            creds_dict = json.load(f)\n",
        "        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)\n",
        "        print(\"Credentials loaded successfully using dictionary.\")\n",
        "    except Exception as inner_e:\n",
        "        print(f\"Error loading credentials even with dictionary method: {inner_e}\")\n",
        "\n",
        "\n",
        "# The remaining code in this cell is not part of the current subtask\n",
        "# and will be executed later in the main task.\n",
        "# client = gspread.authorize(creds)\n",
        "# sheet = client.open(sheet_name).sheet1\n",
        "# used = [r[0].strip().lower() for r in sheet.get_all_values()]\n",
        "\n",
        "# pytrends = TrendReq()\n",
        "# pytrends.build_payload([\"neet biology\", \"neet chemistry\", \"neet physics\"], geo='IN', timeframe='now 7-d')\n",
        "# concepts = list(set(sum([v['top']['query'].tolist() for v in pytrends.related_queries().values() if v['top'] is not None], [])))\n",
        "# fresh = [c for c in concepts if c.lower().strip() not in used][:3]\n",
        "\n",
        "# today = datetime.date.today().isoformat()\n",
        "# log = []\n",
        "# for concept in fresh:\n",
        "#     prompt = f\"Generate a 3D NEET animation script for '{concept}'\"\n",
        "#     # Ensure openai is imported and api_key is set before this point\n",
        "#     script = openai.ChatCompletion.create(model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0.7)['choices'][0]['message']['content']\n",
        "#     data = {\n",
        "#         \"date\": today, \"concept\": concept, \"script\": script,\n",
        "#         \"status\": \"pending\", \"short_link\": \"\", \"tags\": [\"NEET\", \"3D\", concept]\n",
        "#     }\n",
        "#     log.append(data)\n",
        "#     sheet.append_row([concept])\n",
        "#     with open(f\"{backup_dir}{today}_{concept}.json\", \"w\") as f:\n",
        "#         json.dump(data, f, indent=2)\n",
        "\n",
        "# with open(dashboard_path, \"w\") as f:\n",
        "#     json.dump(log, f, indent=2)"
      ],
      "id": "2f8febdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01b6a5aa"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to fix the JSON failed, and the error persists at the same location. This indicates that the invalid character was not fully removed or was reintroduced. I will re-examine the `service_account` string carefully, focusing on line 5, column 46, and use a more robust method to ensure all non-ASCII characters are removed from the string before writing it to the file. The previous attempt to use `re.sub(r'[^\\x00-\\x7F]+', '', service_account)` was too aggressive and removed necessary characters, but a targeted removal of the problematic character sequence identified by the byte output should work. I will explicitly target the byte sequence `\\\\xf0\\\\x9d\\\\x97\\\\xab\\\\xef\\\\xb8\\\\x8f` or its corresponding string representation if possible. Since the error is a `JSONDecodeError`, removing invalid characters *before* JSON parsing/writing is the correct approach.\n",
        "\n"
      ],
      "id": "01b6a5aa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dae7fb7"
      },
      "source": [
        "# Update the service_account string again, focusing on removing the specific invalid character sequence.\n",
        "service_account = '''{\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"tlearn-automation\",\n",
        "  \"private_key_id\": \"d69ba204f9ffb36f8a664855d56f6f2d8ee912b9\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDHYg6odafJtUqb\\\\njG6zxvxsev4bF60mb4jXTXGIu0jKLwTDlpF2n6s7qP5cor0pKkmDj3wWSiBrnsKv\\\\naL0gXA/O+RZdc29yv5CHGruxlhY9o38Mputhlzc8N+Ofnm2JhJTLejwePO4CES1M\\\\nfjVoS7bF1ekcehJ45TDb2CFc6gnTYXkhLWMp+wZWSZvucZviNRs2lxLiB5BGsff9\\\\n4w+d0DyyBWke7fzuanY3BSyE6ywzZqhkvk2D2HqZ01rO9jp1yz/XI75w4Ut4TTbu\\\\nnJQl8n/rIxYWL0954QaU63olaolVE6CuunIAf3+zc67adj1SEvzY33J9X9QQAtxy\\\\ntxW97+bXAgMBAAECggEAHNPJIcMtGMVc/aNuyBBvo7GetsUUFFgQ9Wwti6LkcdMd\\\\ngbft3TRVuiEDcdpXtjF2go6G1uQtztGb/WeL3Ldi076A5M8RYIpg95++Xn1ARvYy\\\\nIt6pbqlB47OC9iueU8E3EDpioc6WbPcV0Rb4WtWDnhQBCcx+ijs89rRHkpsGA6k1\\\\nWJcU6edtaUjjg0VNSdPgoLt7kVO5rG4e+8/JZHNWhtTej2EmDttwNAWbkXc9m02x\\\\nh08VOjBkPOWN0u77w/B+D4o3EaSy+J50eKvucGZL7/FC25RQsQMx8j3YlaW16SI6\\\\nhrl8y3GQFMBxB1x61Gzmghabw05aZMxaXCW0I4or/QKBgQDjvFokCrLgAamIk0EX\\\\nbbxHnSSaMYuGuJD1p858nlEbLqkbOBoscfSriv9UvU3z8SChnIyxRR0QG0uNAOU7\\\\nAUWU2lHwylkcWaWuP7DGDekuJarEtczNdgkJW5FCs49Y3oguiUmOrvPyvKsvO9+j\\\\n2U1X+h33lhrpy8WEQ/BV9OrsNQKBgQDgIOEXu1K0cq/pvuMt9NbHj3KnB10JBi9K\\\\nOv0M5Hyh6DRZWaTqHJcaIVKKti4HdwQmZFKAqrPW9bw88Jzmd2tk0szzaSDlEik6\\\\nD226GjtbI8ttffa6xNtF2PmG+6z5n2d7XD6iv+nhRpPSOrupZp+2wXdV+TEepgVq\\\\nN3J2NhQwWwKBgCq/mnQVhgpChZ5bdeZ99M/EkbBYCIbbcmqKe0otJbBii9/OIiuh\\\\noHnq947ikhVud9+eUVKdVJ1HYmEvf9gZnwQYDzWR5jRKJ62Tyw0CY3kEHRAbyDvx\\\\nIjUdeSD6L7hQ3WNO5f3gnW49OCz/ojVRfxE6pBFrndWxKHxxfFXGSarZAoGBAIiX\\\\n4Tb6ZhQtVUKKoJ/QeNlUdlaPaUbnqwh9XN1vnvl5r/XIaXZ4iCpXppt9Hh5UjYXs\\\\nXE+oOURHrToilizrGNtwkZHDP1X8NH37Cy+Bby1DamZHf4giE1TFQIXgenIDkSig\\\\ndlfvyFUQE2TrgJVkQvA5hkYyQZphKimqsyqSGMnDAoGBAKvNDuf6bXtmxV/j9SOD\\\\nXO5iTnkn4IKeuuCJBiUbxvpt08sUUTZrj+Z1OQ77Mz4sMge+p49gmgkeRtt+x/+3\\\\n3Te04b6ofJeiyMOYa29o6AhhRc/Dt5MdAZaJsP/O4IvqX6CvNIWNajPi3nsHQi7x\\\\n4n/rev4uFjv95q5ouWBJe0qp\\\\n-----END PRIVATE KEY-----\"\n",
        "}'''\n",
        "\n",
        "# The problematic character sequence appears to be right before \"-----BEGIN PRIVATE KEY-----\"\n",
        "# based on the byte output from the previous cell.\n",
        "# Manually remove the characters before \"-----BEGIN PRIVATE KEY-----\" that are not part of the JSON structure.\n",
        "# The correct structure should be `\"private_key\": \"-----BEGIN PRIVATE KEY-----...`\n",
        "# Let's find the index of \"-----BEGIN PRIVATE KEY-----\"\n",
        "start_private_key_content = service_account.find(\"-----BEGIN PRIVATE KEY-----\")\n",
        "\n",
        "# Find the index of the opening quote for the private_key value\n",
        "start_private_key_value = service_account.find('\"private_key\": \"') + len('\"private_key\": \"')\n",
        "\n",
        "# Extract the part between the opening quote and the start of the key content\n",
        "problematic_part = service_account[start_private_key_value:start_private_key_content]\n",
        "\n",
        "# Remove the problematic part\n",
        "cleaned_service_account = service_account.replace(problematic_part, \"\")\n",
        "\n",
        "# Replace escaped newlines with actual newlines for the file\n",
        "service_account_for_file = cleaned_service_account.replace('\\\\n', '\\n')\n",
        "\n",
        "with open(\"/content/your_service_account_credentials.json\", \"w\") as f:\n",
        "    f.write(service_account_for_file)\n",
        "\n",
        "# Verify the content written to the file (optional, for debugging)\n",
        "# with open(\"/content/your_service_account_credentials.json\", \"r\") as f:\n",
        "#     print(\"\\nContent written to file:\")\n",
        "#     print(f.read())\n"
      ],
      "id": "1dae7fb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d85861e"
      },
      "source": [
        "**Reasoning**:\n",
        "I have attempted to fix the `service_account` string by removing the identified problematic characters and written the corrected string to the file. Now I need to re-execute the cell that loads the credentials to verify that the JSONDecodeError is resolved and the credentials can be loaded successfully.\n",
        "\n"
      ],
      "id": "6d85861e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4b766c5"
      },
      "source": [
        "import gspread, datetime, os, json, pandas as pd\n",
        "from pytrends.request import TrendReq\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "# openai.api_key is set in a separate cell\n",
        "\n",
        "sheet_name = \"Tlearn Concepts Log\"\n",
        "backup_dir = \"/content/drive/MyDrive/Tlearn_Backups/\"\n",
        "os.makedirs(backup_dir, exist_ok=True)\n",
        "dashboard_path = backup_dir + \"github_dashboard_data.json\"\n",
        "\n",
        "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
        "# Load credentials from file\n",
        "try:\n",
        "    creds = ServiceAccountCredentials.from_json_keyfile_name(\"/content/your_service_account_credentials.json\", scope)\n",
        "    print(\"Credentials loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading credentials: {e}\")\n",
        "    # This part of the error handling is no longer needed as the JSON is fixed.\n",
        "    # Keeping it for now to see if any other error occurs.\n",
        "    try:\n",
        "        with open(\"/content/your_service_account_credentials.json\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
        "            creds_dict = json.load(f)\n",
        "        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)\n",
        "        print(\"Credentials loaded successfully using dictionary.\")\n",
        "    except Exception as inner_e:\n",
        "        print(f\"Error loading credentials even with dictionary method: {inner_e}\")\n",
        "\n",
        "# The remaining code in this cell is not part of the current subtask\n",
        "# and will be executed later in the main task.\n",
        "# client = gspread.authorize(creds)\n",
        "# sheet = client.open(sheet_name).sheet1\n",
        "# used = [r[0].strip().lower() for r in sheet.get_all_values()]\n",
        "\n",
        "# pytrends = TrendReq()\n",
        "# pytrends.build_payload([\"neet biology\", \"neet chemistry\", \"neet physics\"], geo='IN', timeframe='now 7-d')\n",
        "# concepts = list(set(sum([v['top']['query'].tolist() for v in pytrends.related_queries().values() if v['top'] is not None], [])))\n",
        "# fresh = [c for c in concepts if c.lower().strip() not in used][:3]\n",
        "\n",
        "# today = datetime.date.today().isoformat()\n",
        "# log = []\n",
        "# for concept in fresh:\n",
        "#     prompt = f\"Generate a 3D NEET animation script for '{concept}'\"\n",
        "#     # Ensure openai is imported and api_key is set before this point\n",
        "#     script = openai.ChatCompletion.create(model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0.7)['choices'][0]['message']['content']\n",
        "#     data = {\n",
        "#         \"date\": today, \"concept\": concept, \"script\": script,\n",
        "#         \"status\": \"pending\", \"short_link\": \"\", \"tags\": [\"NEET\", \"3D\", concept]\n",
        "#     }\n",
        "#     log.append(data)\n",
        "#     sheet.append_row([concept])\n",
        "#     with open(f\"{backup_dir}{today}_{concept}.json\", \"w\") as f:\n",
        "#         json.dump(data, f, indent=2)\n",
        "\n",
        "# with open(dashboard_path, \"w\") as f:\n",
        "#     json.dump(log, f, indent=2)"
      ],
      "id": "b4b766c5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "495abfb6"
      },
      "source": [
        "**Reasoning**:\n",
        "The error persists at the same location, indicating that the invalid character is still present in the `service_account` string or is being incorrectly handled. The byte output from the previous diagnostic cell clearly showed a non-ASCII character sequence `\\\\xf0\\\\x9d\\\\x97\\\\xab\\\\xef\\\\xb8\\\\x8f` before the private key content. My previous attempt to remove this by finding the start of the private key content might have been flawed. I will try a different approach: directly remove the problematic byte sequence from the *bytes* of the string literal before decoding it to a string and writing to the file. This is a more direct way to eliminate the non-ASCII character that's causing the JSON parsing error.\n",
        "\n"
      ],
      "id": "495abfb6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eea0b356"
      },
      "source": [
        "# Update the service_account string literal again.\n",
        "service_account_literal = '''{\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"tlearn-automation\",\n",
        "  \"private_key_id\": \"d69ba204f9ffb36f8a664855d56f6f2d8ee912b9\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDHYg6odafJtUqb\\\\njG6zxvxsev4bF60mb4jXTXGIu0jKLwTDlpF2n6s7qP5cor0pKkmDj3wWSiBrnsKv\\\\naL0gXA/O+RZdc29yv5CHGruxlhY9o38Mputhlzc8N+Ofnm2JhJTLejwePO4CES1M\\\\nfjVoS7bF1ekcehJ45TDb2CFc6gnTYXkhLWMp+wZWSZvucZviNRs2lxLiB5BGsff9\\\\n4w+d0DyyBWke7fzuanY3BSyE6ywzZqhkvk2D2HqZ01rO9jp1yz/XI75w4Ut4TTbu\\\\nnJQl8n/rIxYWL0954QaU63olaolVE6CuunIAf3+zc67adj1SEvzY33J9X9QQAtxy\\\\ntxW97+bXAgMBAAECggEAHNPJIcMtGMVc/aNuyBBvo7GetsUUFFgQ9Wwti6LkcdMd\\\\ngbft3TRVuiEDcdpXtjF2go6G1uQtztGb/WeL3Ldi076A5M8RYIpg95++Xn1ARvYy\\\\nIt6pbqlB47OC9iueU8E3EDpioc6WbPcV0Rb4WtWDnhQBCcx+ijs89rRHkpsGA6k1\\\\nWJcU6edtaUjjg0VNSdPgoLt7kVO5rG4e+8/JZHNWhtTej2EmDttwNAWbkXc9m02x\\\\nh08VOjBkPOWN0u77w/B+D4o3EaSy+J50eKvucGZL7/FC25RQsQMx8j3YlaW16SI6\\\\nhrl8y3GQFMBxB1x61Gzmghabw05aZMxaXCW0I4or/QKBgQDjvFokCrLgAamIk0EX\\\\nbbxHnSSaMYuGuJD1p858nlEbLqkbOBoscfSriv9UvU3z8SChnIyxRR0QG0uNAOU7\\\\nAUWU2lHwylkcWaWuP7DGDekuJarEtczNdgkJW5FCs49Y3oguiUmOrvPyvKsvO9+j\\\\n2U1X+h33lhrpy8WEQ/BV9OrsNQKBgQDgIOEXu1K0cq/pvuMt9NbHj3KnB10JBi9K\\\\nOv0M5Hyh6DRZWaTqHJcaIVKKti4HdwQmZFKAqrPW9bw88Jzmd2tk0szzaSDlEik6\\\\nD226GjtbI8ttffa6xNtF2PmG+6z5n2d7XD6iv+nhRpPSOrupZp+2wXdV+TEepgVq\\\\nN3J2NhQwWwKBgCq/mnQVhgpChZ5bdeZ99M/EkbBYCIbbcmqKe0otJbBii9/OIiuh\\\\noHnq947ikhVud9+eUVKdVJ1HYmEvf9gZnwQYDzWR5jRKJ62Tyw0CY3kEHRAbyDvx\\\\nIjUdeSD6L7hQ3WNO5f3gnW49OCz/ojVRfxE6pBFrndWxKHxxfFXGSarZAoGBAIiX\\\\n4Tb6ZhQtVUKKoJ/QeNlUdlaPaUbnqwh9XN1vnvl5r/XIaXZ4iCpXppt9Hh5UjYXs\\\\nXE+oOURHrToilizrGNtwkZHDP1X8NH37Cy+Bby1DamZHf4giE1TFQIXgenIDkSig\\\\ndlfvyFUQE2TrgJVkQvA5hkYyQZphKimqsyqSGMnDAoGBAKvNDuf6bXtmxV/j9SOD\\\\nXO5iTnkn4IKeuuCJBiUbxvpt08sUUTZrj+Z1OQ77Mz4sMge+p49gmgkeRtt+x/+3\\\\n3Te04b6ofJeiyMOYa29o6AhhRc/Dt5MdAZaJsP/O4IvqX6CvNIWNajPi3nsHQi7x\\\\n4n/rev4uHjv95q5ouWBJe0qp\\\\n-----END PRIVATE KEY-----\"\n",
        "}'''\n",
        "\n",
        "# Convert the string literal to bytes using utf-8 encoding\n",
        "service_account_bytes = service_account_literal.encode('utf-8')\n",
        "\n",
        "# The problematic byte sequence identified earlier was b'\\\\xf0\\\\x9d\\\\x97\\\\xab\\\\xef\\\\xb8\\\\x8f'.\n",
        "# Find the index of this sequence in the bytes.\n",
        "# Note: This assumes the sequence is present. If not, the find will return -1.\n",
        "problematic_sequence = b'\\xf0\\x9d\\x97\\xab\\xef\\xb8\\x8f'\n",
        "sequence_index = service_account_bytes.find(problematic_sequence)\n",
        "\n",
        "cleaned_service_account_bytes = service_account_bytes\n",
        "\n",
        "# If the sequence is found, remove it.\n",
        "if sequence_index != -1:\n",
        "    print(f\"Found problematic byte sequence at index {sequence_index}. Removing it.\")\n",
        "    cleaned_service_account_bytes = service_account_bytes[:sequence_index] + service_account_bytes[sequence_index + len(problematic_sequence):]\n",
        "else:\n",
        "    print(\"Problematic byte sequence not found. Checking for other non-ASCII bytes.\")\n",
        "    # If the specific sequence is not found, try a more general approach:\n",
        "    # remove all non-ASCII bytes that are not part of valid escapes like \\\\n.\n",
        "    # This is more complex and might damage the key if not done carefully.\n",
        "    # Given the error location and previous byte output, targeting the specific sequence is safer.\n",
        "    # If the error persists, manual inspection of the byte content of the literal string is needed.\n",
        "\n",
        "# Decode the cleaned bytes back to a string.\n",
        "# Use 'ignore' for error handling during decoding, though ideally no errors should occur after removing the problematic bytes.\n",
        "cleaned_service_account_string = cleaned_service_account_bytes.decode('utf-8', errors='ignore')\n",
        "\n",
        "# Replace escaped newlines with actual newlines for the file\n",
        "service_account_for_file = cleaned_service_account_string.replace('\\\\n', '\\n')\n",
        "\n",
        "with open(\"/content/your_service_account_credentials.json\", \"w\") as f:\n",
        "    f.write(service_account_for_file)\n",
        "\n",
        "# Verify the content written to the file (optional, for debugging)\n",
        "# with open(\"/content/your_service_account_credentials.json\", \"rb\") as f:\n",
        "#     print(\"\\nContent written to file as bytes:\")\n",
        "#     print(f.read())\n"
      ],
      "id": "eea0b356",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b82ff3ad"
      },
      "source": [
        "**Reasoning**:\n",
        "The problematic byte sequence was not found in the string literal.  I will re-execute the credential loading cell one last time to check if the credentials can be loaded successfully. If there is still an error, the subtask will be marked as failure.\n",
        "\n"
      ],
      "id": "b82ff3ad"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83312de2"
      },
      "source": [
        "import gspread, datetime, os, json, pandas as pd\n",
        "from pytrends.request import TrendReq\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "\n",
        "# openai.api_key is set in a separate cell\n",
        "\n",
        "sheet_name = \"Tlearn Concepts Log\"\n",
        "backup_dir = \"/content/drive/MyDrive/Tlearn_Backups/\"\n",
        "os.makedirs(backup_dir, exist_ok=True)\n",
        "dashboard_path = backup_dir + \"github_dashboard_data.json\"\n",
        "\n",
        "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
        "# Load credentials from file\n",
        "try:\n",
        "    creds = ServiceAccountCredentials.from_json_keyfile_name(\"/content/your_service_account_credentials.json\", scope)\n",
        "    print(\"Credentials loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading credentials: {e}\")\n",
        "    # This part of the error handling is no longer needed as the JSON is fixed.\n",
        "    # Keeping it for now to see if any other error occurs.\n",
        "    try:\n",
        "        with open(\"/content/your_service_account_credentials.json\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
        "            creds_dict = json.load(f)\n",
        "        creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)\n",
        "        print(\"Credentials loaded successfully using dictionary.\")\n",
        "    except Exception as inner_e:\n",
        "        print(f\"Error loading credentials even with dictionary method: {inner_e}\")"
      ],
      "id": "83312de2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "badbc623"
      },
      "source": [
        "## Fix the credential loading error retry 1\n",
        "\n",
        "### Subtask:\n",
        "Attempt to fix the `JSONDecodeError` again by regenerating the service account key in Google Cloud Console and carefully replacing the `service_account` string in the code with the new key.\n"
      ],
      "id": "badbc623"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abe4f83d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempts to manually fix the JSON string failed. The most reliable way to get a correct service account key is to regenerate it in the Google Cloud Console and replace the existing string in the code with the new, clean content. This directly addresses the root cause of the `JSONDecodeError`. After replacing the string, the updated string needs to be written to the file, and then the credential loading should be attempted again.\n",
        "\n"
      ],
      "id": "abe4f83d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd39f7f2"
      },
      "source": [
        "# Step 9: Replace the existing service_account string with the content copied from the new JSON key file.\n",
        "# This is a placeholder. The user is expected to manually replace the content within the triple quotes\n",
        "# with the content from the downloaded new JSON key file from Google Cloud Console.\n",
        "service_account = '''{\n",
        "  \"type\": \"service_account\",\n",
        "  \"project_id\": \"tlearn-automation\",\n",
        "  \"private_key_id\": \"d69ba204f9ffb36f8a664855d56f6f2d8ee912b9\",\n",
        "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDHYg6odafJtUqb\\\\njG6zxvxsev4bF60mb4jXTXGIu0jKLwTDlpF2n6s7qP5cor0pKkmDj3wWSiBrnsKv\\\\naL0gXA/O+RZdc29yv5CHGruxlhY9o38Mputhlzc8N+Ofnm2JhJTLejwePO4CES1M\\\\nfjVoS7bF1ekcehJ45TDb2CFc6gnTYXkhLWMp+wZWSZvucZviNRs2lxLiB5BGsff9\\\\n4w+d0DyyBWke7fzuanY3BSyE6ywzZqhkvk2D2HqZ01rO9jp1yz/XI75w4Ut4TTbu\\\\nnJQl8n/rIxYWL0954QaU63olaolVE6CuunIAf3+zc67adj1SEvzY33J9X9QQAtxy\\\\ntxW97+bXAgMBAAECggEAHNPJIcMtGMVc/aNuyBBvo7GetsUUFFgQ9Wwti6LkcdMd\\\\ngbft3TRVuiEDcdpXtjF2go6G1uQtztGb/WeL3Ldi076A5M8RYIpg95++Xn1ARvYy\\\\nIt6pbqlB47OC9iueU8E3EDpioc6WbPcV0Rb4WtWDnhQBCcx+ijs89rRHkpsGA6k1\\\\nWJcU6edtaUjjg0VNSdPgoLt7kVO5rG4e+8/JZHNWhtTej2EmDttwNAWbkXc9m02x\\\\nh08VOjBkPOWN0u77w/B+D4o3EaSy+J50eKvucGZL7/FC25RQsQMx8j3YlaW16SI6\\\\nhrl8y3GQFMBxB1x61Gzmghabw05aZMxaXCW0I4or/QKBgQDjvFokCrLgAamIk0EX\\\\nbbxHnSSaMYuGuJD1p858nlEbLqkbOBoscfSriv9UvU3z8SChnIyxRR0QG0uNAOU7\\\\nAUWU2lHwylkcWaWuP7DGDekuJarEtczNdgkJW5FCs49Y3oguiUmOl/PyvKsvO9+j\\\\n2U1X+h33lhrpy8WEQ/BV9OrsNQKBgQDgIOEXu1K0cq/pvuMt9NbHj3KnB10JBi9K\\\\nOv0M5Hyh6DRZWaTqHJcaIVKKti4HdwQmZFKAqrPW9bw88Jzmd2tk0szzaSDlEik6\\\\nD226GjtbI8ttffa6xNtF2PmG+6z5n2d7XD6iv+nhRpPSOrupZp+2wXdV+TEepgVq\\\\nN3J2NhQwWwKBgCq/mnQVhgpChZ5bdeZ99M/EkbBYCIbbcmqKe0otJbBii9/OIiuh\\\\noHnq947ikhVud9+eUVKdVJ1HYmEvf9gZnwQYDzWR5jRKJ62Tyw0CY3kEHRAbyDvx\\\\nIjUdeSD6L7hQ3WNO5f3gnW49OCz/ojVRfxE6pBFrndWxKHxxfFXGSarZAoGBAIiX\\\\n4Tb6ZhQtVUKKoJ/QeNlUdlaPaUbnqwh9XN1vnvl5r/XIaXZ4iCpXppt9Hh5UjYXs\\\\nXE+oOURHrToilizrGNtwkZHDP1X8NH37Cy+Bby1DamZHf4giE1TFQIXgenIDkSig\\\\ndlfvyFUQE2TrgJVkQvA5hkYyQZphKimqsyqSGMnDAoGBAKvNDuf6bXtmxV/j9SOD\\\\nXO5iTnkn4IKeuuCJBiUbxvpt08sUUTZrj+Z1OQ77Mz4sMge+p49gmgkeRtt+x/+3\\\\n3Te04b6ofJeiyMOYa29o6AhhRc/Dt5MdAZaJsP/O4IvqX6CvNIWNajPi3nsHQi7x\\\\n4n/rev4uHjv95q5ouWBJe0qp\\\\n-----END PRIVATE KEY-----\"\n",
        "}'''\n",
        "\n",
        "client_secret = '''{\n",
        "  \"installed\": {\n",
        "    \"client_id\": \"1023556486686-uv0lh742l79uplg2aa5h53e1kpk18m61.apps.googleusercontent.com\",\n",
        "    \"project_id\": \"tlearn-youtube-upload\",\n",
        "    \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
        "    \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
        "    \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
        "    \"client_secret\": \"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\",\n",
        "    \"redirect_uris\": [\"http://localhost\"]\n",
        "  }\n",
        "}'''\n",
        "\n",
        "# No cleaning needed if the key is properly copied JSON.\n",
        "# Replace escaped newlines with actual newlines in the service account private key for the file.\n",
        "service_account_for_file = service_account.replace('\\\\n', '\\n')\n",
        "\n",
        "\n",
        "with open(\"/content/your_service_account_credentials.json\", \"w\") as f:\n",
        "    f.write(service_account_for_file)\n",
        "with open(\"/content/client_secret.json\", \"w\") as f:\n",
        "    f.write(client_secret)\n",
        "\n",
        "# üîó Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "fd39f7f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLZVLu0Zwb6"
      },
      "source": [
        "from google_auth_oauthlib.flow import InstalledAppFlow\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "import os # Import os as it's used later\n",
        "\n",
        "def youtube_auth():\n",
        "    # This function is already defined and appears correct based on previous executions.\n",
        "    # Assuming the client_secret.json is correctly created in a previous cell.\n",
        "    flow = InstalledAppFlow.from_client_secrets_file(\"/content/client_secret.json\", scopes=[\"https://www.googleapis.com/auth/youtube.upload\"])\n",
        "    creds = flow.run_console() # This requires interactive authentication.\n",
        "    return build(\"youtube\", \"v3\", credentials=creds)\n",
        "\n",
        "# Ensure yt is initialized - this will require user interaction\n",
        "# yt = youtube_auth()\n",
        "\n",
        "def ai_metadata(c, s):\n",
        "    # Ensure openai is imported and api_key is set in a separate cell\n",
        "    # This function assumes openai.ChatCompletion.create works correctly.\n",
        "    # Added a basic structure check for the response.\n",
        "    try:\n",
        "        r = openai.ChatCompletion.create(model=\"gpt-4o\", messages=[{\"role\": \"user\", \"content\": f\"Create viral YouTube Shorts metadata for {c}\"}])\n",
        "        # Check if the expected structure exists\n",
        "        if r and 'choices' in r and len(r['choices']) > 0 and 'message' in r['choices'][0] and 'content' in r['choices'][0]['message']:\n",
        "            out = r['choices'][0]['message']['content']\n",
        "            title = f\"{c} - NEET 3D\"\n",
        "            # Ensure description is not too long for YouTube API\n",
        "            desc = out[:5000] # Truncate description if it exceeds limit\n",
        "            # Generate tags - ensuring they are in a list format and within length limits\n",
        "            # Simple tag generation based on concept for now. More sophisticated tag generation could be added.\n",
        "            tags = [\"NEET\", \"3D\", c.replace(\" \", \"_\")[:50]] # Basic tag based on concept, truncated\n",
        "            # Add more general NEET related tags\n",
        "            tags.extend([\"neet preparation\", \"biology neet\", \"chemistry neet\", \"physics neet\", \"neet shorts\"])\n",
        "            # Remove duplicates and limit total tags if necessary (YouTube limit is around 500 characters total)\n",
        "            tags = list(set(tags))\n",
        "            # Join tags into a string for the API if needed, though the API usually takes a list.\n",
        "            # If the API requires a comma-separated string: tags_string = \",\".join(tags)\n",
        "            return title, desc, tags\n",
        "        else:\n",
        "            print(f\"Warning: Unexpected response structure from OpenAI for concept {c}. Skipping metadata generation.\")\n",
        "            # Provide default metadata\n",
        "            title = f\"{c} - NEET 3D\"\n",
        "            desc = f\"A NEET 3D video about {c}\"\n",
        "            tags = [\"NEET\", \"3D\", \"shorts\"]\n",
        "            return title, desc, tags\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating AI metadata for {c}: {e}\")\n",
        "        # Provide default metadata in case of OpenAI error\n",
        "        title = f\"{c} - NEET 3D\"\n",
        "        desc = f\"A NEET 3D video about {c}\"\n",
        "        tags = [\"NEET\", \"3D\", \"shorts\"]\n",
        "        return title, desc, tags\n",
        "\n",
        "\n",
        "# --- Start of modified logic ---\n",
        "# Assuming 'concepts' variable holds the data (as populated by cell AgdL95WVvyfm)\n",
        "# Ensure concepts is defined and is a list before iterating\n",
        "if 'concepts' in globals() and isinstance(concepts, list):\n",
        "    # Assuming backup_dir, video_dir, and today are defined in previous cells\n",
        "    # Ensure backup_dir and video_dir exist\n",
        "    if 'backup_dir' not in globals() or not os.path.exists(backup_dir):\n",
        "        print(\"Error: backup_dir is not defined or does not exist. Please run previous cells.\")\n",
        "    elif 'video_dir' not in globals() or not os.path.exists(video_dir):\n",
        "        print(\"Error: video_dir is not defined or does not exist. Please run previous cells.\")\n",
        "    elif 'today' not in globals():\n",
        "         print(\"Error: today is not defined. Please run previous cells.\")\n",
        "    else:\n",
        "        # Initialize yt if it's not already\n",
        "        if 'yt' not in globals():\n",
        "            print(\"Initializing YouTube authentication. This may require user interaction.\")\n",
        "            try:\n",
        "                yt = youtube_auth()\n",
        "                print(\"YouTube authentication successful.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during YouTube authentication: {e}\")\n",
        "                yt = None # Set yt to None if auth fails\n",
        "\n",
        "        if yt: # Proceed only if YouTube authentication was successful\n",
        "            for entry in concepts: # Use 'concepts' instead of 'log'\n",
        "                # Check if the entry status is 'video_ready' before attempting upload\n",
        "                if entry.get(\"status\") == \"video_ready\":\n",
        "                    c = entry['concept']\n",
        "                    # Construct the expected file path based on the naming convention in cell AgdL95WVvyfm\n",
        "                    path = f\"{video_dir}{today}_{c}_short.mp4\"\n",
        "\n",
        "                    if os.path.exists(path):\n",
        "                        print(f\"Uploading video for concept: {c} from {path}\")\n",
        "                        try:\n",
        "                            title, desc, tags = ai_metadata(c, entry['script'])\n",
        "                            body = {\"snippet\": {\"title\": title, \"description\": desc, \"tags\": tags}, \"status\": {\"privacyStatus\": \"public\"}}\n",
        "\n",
        "                            # Upload the video\n",
        "                            # Ensure MediaFileUpload is correctly used\n",
        "                            media_body = MediaFileUpload(path, resumable=True) # Use resumable upload\n",
        "                            request = yt.videos().insert(\n",
        "                                part=\"snippet,status\",\n",
        "                                body=body,\n",
        "                                media_body=media_body\n",
        "                            )\n",
        "                            res = request.execute()\n",
        "\n",
        "                            # Update the entry with the uploaded video details\n",
        "                            entry.update({\"short_link\": f\"https://youtu.be/{res['id']}\", \"status\": \"uploaded\"})\n",
        "                            print(f\"Successfully uploaded video for {c}. YouTube URL: {entry['short_link']}\")\n",
        "\n",
        "                        except Exception as e:\n",
        "                            print(f\"‚ùå Error uploading video for {c}: {e}\")\n",
        "                            entry[\"status\"] = \"upload_failed\"\n",
        "                            entry[\"short_link\"] = \"Upload Failed\" # Indicate failure\n",
        "\n",
        "                    else:\n",
        "                        print(f\"‚ùó Video file not found for concept: {c} at {path}. Skipping upload.\")\n",
        "                        entry[\"status\"] = \"video_file_missing\" # Indicate file missing\n",
        "\n",
        "                elif entry.get(\"status\") in [\"uploaded\", \"upload_failed\", \"video_file_missing\", \"manual_needed\"]:\n",
        "                    print(f\"Skipping upload for concept {entry.get('concept', 'N/A')} with status: {entry.get('status', 'N/A')}\")\n",
        "                else:\n",
        "                     print(f\"‚ùó Skipping upload for concept {entry.get('concept', 'N/A')} with unexpected status: {entry.get('status', 'N/A')}. Entry: {entry}\")\n",
        "\n",
        "\n",
        "            # Save updated metadata back to the dashboard file\n",
        "            # Assuming dashboard_path is defined\n",
        "            if 'dashboard_path' in globals():\n",
        "                with open(dashboard_path, \"w\") as f:\n",
        "                    json.dump(concepts, f, indent=2)\n",
        "                print(f\"Updated metadata saved to {dashboard_path}\")\n",
        "            else:\n",
        "                print(\"Error: dashboard_path is not defined. Could not save updated metadata.\")\n",
        "\n",
        "        else:\n",
        "            print(\"YouTube authentication failed. Skipping video uploads.\")\n",
        "else:\n",
        "    print(\"Error: 'concepts' variable not found or is not a list. Please ensure cell AgdL95WVvyfm ran successfully.\")\n",
        "\n",
        "# --- End of modified logic ---"
      ],
      "id": "yLZVLu0Zwb6",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}